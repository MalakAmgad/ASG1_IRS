{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174728a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read user-item rating matrix\n",
    "#item-based CF\n",
    "#normalizee rating by mean-centring for each user Su,p=ru,p - mean(ratings of user u)\n",
    "#calculate similarity for co-rated items (items rated by common users)\n",
    "#compute item-item similarity (adjusted cosine, pearson, jaccard)\n",
    "# k closest items highly similar items to target item\n",
    "#calculate predicted rating using wheighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380055e",
   "metadata": {},
   "source": [
    "# Malak Amgad 221100451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4ca29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply item-based collaborative filtering using Cosine similarity with mean-centering.\n",
    "#Identify the top 20% of similar items for each target item.\n",
    "#Predict missing ratings using these items.\n",
    "#Compute DF and DS.\n",
    "#Select top 20% items using DS.\n",
    "#Use these for updated rating predictions.\n",
    "#Compare similarity lists from steps 2 and 5. Provide commentary.\n",
    "#Compare predicted ratings from steps 3 and 6. Discuss.\n",
    "#Give your comments in a separate section in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46549d5",
   "metadata": {},
   "source": [
    "#### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e034816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.sparse import csr_matrix, save_npz ,load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4894e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20M ratings... (This may take a minute)\n",
      "Converting 0.5 ratings to 1.0...\n",
      "Found 0 ratings that are 0.5.\n",
      "Saving to ../../../Dataset/ratings_modified.csv...\n",
      "Done! Transformation complete.\n",
      "Number of 0.5 ratings remaining: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file_path = '../../Dataset/ratings.csv'\n",
    "output_file_path = '../../Dataset/ratings_modified.csv'\n",
    "\n",
    "print(\"Loading 20M ratings... (This may take a minute)\")\n",
    "df = pd.read_csv(\n",
    "    input_file_path, \n",
    "    dtype={'userId': 'int32', 'movieId': 'int32', 'rating': 'float32'}\n",
    ")\n",
    "print(\"Converting 0.5 ratings to 1.0...\")\n",
    "count_changes = len(df[df['rating'] == 0.5])\n",
    "print(f\"Found {count_changes} ratings that are 0.5.\")\n",
    "df.loc[df['rating'] == 0.5, 'rating'] = 1.0\n",
    "print(f\"Saving to {output_file_path}...\")\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Done! Transformation complete.\")\n",
    "remaining_0_5 = len(df[df['rating'] == 0.5])\n",
    "print(f\"Number of 0.5 ratings remaining: {remaining_0_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907fb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Matrix Dimensions: 138494 users x 131263 movies\n",
      "Creating sparse matrix...\n",
      "Directory created/verified: Output\n",
      "Saved successfully to: Output\\matrix.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# renamed the modified ratings file to ratings.csv\n",
    "def create_raw_id_matrix(csv_file):\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(csv_file, usecols=['userId', 'movieId', 'rating'])\n",
    "    \n",
    "    n_users = df['userId'].max() + 1\n",
    "    n_movies = df['movieId'].max() + 1\n",
    "\n",
    "    print(f\"Matrix Dimensions: {n_users} users x {n_movies} movies\")\n",
    "\n",
    "    print(\"Creating sparse matrix...\")\n",
    "    matrix = csr_matrix(\n",
    "        (df['rating'], (df['userId'], df['movieId'])), \n",
    "        shape=(n_users, n_movies)\n",
    "    )\n",
    "\n",
    "    return matrix\n",
    "\n",
    "matrix = create_raw_id_matrix('../../Dataset/ratings.csv')\n",
    "output_folder = os.path.join('Output')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"Directory created/verified: {output_folder}\")\n",
    "output_file = os.path.join(output_folder, 'matrix.npz')\n",
    "save_npz(output_file, matrix)\n",
    "\n",
    "print(f\"Saved successfully to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c4a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Width (Max ID): 131263\n",
      "Actual Movies with Ratings: 26744\n"
     ]
    }
   ],
   "source": [
    "actual_movie_count = (matrix.getnnz(axis=0) > 0).sum()\n",
    "\n",
    "print(f\"Matrix Width (Max ID): {matrix.shape[1]}\")\n",
    "print(f\"Actual Movies with Ratings: {actual_movie_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f17eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read user-item rating matrix\n",
    "from scipy.sparse import load_npz\n",
    "loaded_matrix = load_npz(r'Output/matrix.npz')\n",
    "matrix = loaded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5d1d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0       1       2       3       4       5       6       7       8       \\\n",
      "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1     0.0     0.0     3.5     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2     0.0     0.0     0.0     4.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3     0.0     4.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4     0.0     0.0     0.0     0.0     0.0     0.0     3.0     0.0     0.0   \n",
      "\n",
      "   9       ...  131253  131254  131255  131256  131257  131258  131259  \\\n",
      "0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   131260  131261  131262  \n",
      "0     0.0     0.0     0.0  \n",
      "1     0.0     0.0     0.0  \n",
      "2     0.0     0.0     0.0  \n",
      "3     0.0     0.0     0.0  \n",
      "4     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 131263 columns]\n"
     ]
    }
   ],
   "source": [
    "first_5_rows = matrix[:5]\n",
    "dense_view = first_5_rows.toarray()\n",
    "df_view = pd.DataFrame(dense_view)\n",
    "print(df_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4838d8",
   "metadata": {},
   "source": [
    "#### Apply colaborative filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22695ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating means...\n",
      "Saving to Output\\user_means.csv...\n",
      "Done! First 5 user means:\n",
      "   user_idx  mean_rating\n",
      "0         0     0.000000\n",
      "1         1     3.742857\n",
      "2         2     4.000000\n",
      "3         3     4.122995\n",
      "4         4     3.571429\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(output_folder, 'user_means.csv')\n",
    "print(\"Calculating means...\")\n",
    "user_sums = np.array(matrix.sum(axis=1)).flatten()\n",
    "user_counts = matrix.getnnz(axis=1)\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    user_means = user_sums / user_counts\n",
    "    user_means[~np.isfinite(user_means)] = 0.0\n",
    "df_means = pd.DataFrame({\n",
    "    'user_idx': np.arange(len(user_means)),\n",
    "    'mean_rating': user_means\n",
    "})\n",
    "\n",
    "print(f\"Saving to {output_csv_path}...\")\n",
    "df_means.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Done! First 5 user means:\")\n",
    "print(df_means.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f625e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding means to match matrix structure...\n",
      "Applying mean-centering...\n",
      "Successfully saved normalized matrix to: Output\\normalized_matrix.npz\n"
     ]
    }
   ],
   "source": [
    "base_path = output_folder\n",
    "raw_matrix_path = os.path.join(base_path, 'matrix.npz')\n",
    "means_csv_path = os.path.join(base_path, 'user_means.csv')\n",
    "output_path = os.path.join(base_path, 'normalized_matrix.npz')\n",
    "\n",
    "matrix = load_npz(raw_matrix_path)\n",
    "df_means = pd.read_csv(means_csv_path)\n",
    "df_means = df_means.sort_values('user_idx')\n",
    "user_means_array = df_means['mean_rating'].values\n",
    "print(\"Expanding means to match matrix structure...\")\n",
    "user_counts = matrix.getnnz(axis=1)\n",
    "expanded_means = np.repeat(user_means_array, user_counts)\n",
    "print(\"Applying mean-centering...\")\n",
    "matrix.data = matrix.data - expanded_means\n",
    "save_npz(output_path, matrix)\n",
    "print(f\"Successfully saved normalized matrix to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f3e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 2)\t-0.24285714285714288\n",
      "  (1, 29)\t-0.24285714285714288\n",
      "  (1, 32)\t-0.24285714285714288\n",
      "  (1, 47)\t-0.24285714285714288\n",
      "  (1, 50)\t-0.24285714285714288\n",
      "  (1, 112)\t-0.24285714285714288\n",
      "  (1, 151)\t0.2571428571428571\n",
      "  (1, 223)\t0.2571428571428571\n",
      "  (1, 253)\t0.2571428571428571\n",
      "  (1, 260)\t0.2571428571428571\n",
      "  (1, 293)\t0.2571428571428571\n",
      "  (1, 296)\t0.2571428571428571\n",
      "  (1, 318)\t0.2571428571428571\n",
      "  (1, 337)\t-0.24285714285714288\n",
      "  (1, 367)\t-0.24285714285714288\n",
      "  (1, 541)\t0.2571428571428571\n",
      "  (1, 589)\t-0.24285714285714288\n",
      "  (1, 593)\t-0.24285714285714288\n",
      "  (1, 653)\t-0.7428571428571429\n",
      "  (1, 919)\t-0.24285714285714288\n",
      "  (1, 924)\t-0.24285714285714288\n",
      "  (1, 1009)\t-0.24285714285714288\n",
      "  (1, 1036)\t0.2571428571428571\n",
      "  (1, 1079)\t0.2571428571428571\n",
      "  (1, 1080)\t-0.24285714285714288\n",
      "  :\t:\n",
      "  (1, 6755)\t-0.24285714285714288\n",
      "  (1, 6774)\t0.2571428571428571\n",
      "  (1, 6807)\t-0.24285714285714288\n",
      "  (1, 6834)\t-0.24285714285714288\n",
      "  (1, 6888)\t-0.7428571428571429\n",
      "  (1, 7001)\t-0.24285714285714288\n",
      "  (1, 7045)\t-0.24285714285714288\n",
      "  (1, 7046)\t0.2571428571428571\n",
      "  (1, 7153)\t1.2571428571428571\n",
      "  (1, 7164)\t-0.24285714285714288\n",
      "  (1, 7247)\t-0.24285714285714288\n",
      "  (1, 7387)\t-0.24285714285714288\n",
      "  (1, 7389)\t0.2571428571428571\n",
      "  (1, 7438)\t0.2571428571428571\n",
      "  (1, 7449)\t-0.24285714285714288\n",
      "  (1, 7454)\t0.2571428571428571\n",
      "  (1, 7482)\t-0.7428571428571429\n",
      "  (1, 7757)\t0.2571428571428571\n",
      "  (1, 8368)\t0.2571428571428571\n",
      "  (1, 8482)\t-0.24285714285714288\n",
      "  (1, 8507)\t1.2571428571428571\n",
      "  (1, 8636)\t0.7571428571428571\n",
      "  (1, 8690)\t-0.24285714285714288\n",
      "  (1, 8961)\t0.2571428571428571\n",
      "  (1, 31696)\t0.2571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(matrix[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09ee0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0         1         2       3       4       5         6       7       \\\n",
      "0     0.0  0.000000  0.000000     0.0     0.0     0.0  0.000000     0.0   \n",
      "1     0.0  0.000000 -0.242857     0.0     0.0     0.0  0.000000     0.0   \n",
      "2     0.0  0.000000  0.000000     0.0     0.0     0.0  0.000000     0.0   \n",
      "3     0.0 -0.122995  0.000000     0.0     0.0     0.0  0.000000     0.0   \n",
      "4     0.0  0.000000  0.000000     0.0     0.0     0.0 -0.571429     0.0   \n",
      "\n",
      "   8       9       ...  131253  131254  131255  131256  131257  131258  \\\n",
      "0     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   131259  131260  131261  131262  \n",
      "0     0.0     0.0     0.0     0.0  \n",
      "1     0.0     0.0     0.0     0.0  \n",
      "2     0.0     0.0     0.0     0.0  \n",
      "3     0.0     0.0     0.0     0.0  \n",
      "4     0.0     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 131263 columns]\n"
     ]
    }
   ],
   "source": [
    "first_5_rows = matrix[:5]\n",
    "dense_view = first_5_rows.toarray()\n",
    "df_view = pd.DataFrame(dense_view)\n",
    "print(df_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c86602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw matrix...\n",
      "Calculating stats...\n",
      "Saving to Output\\item_averages_with_counts.csv...\n",
      "\n",
      "The 2 items with lowest rating (prioritizing most rated):\n",
      "        item_idx  avg_rating  rating_count\n",
      "107704    107704         1.0            11\n",
      "56835      56835         1.0             6\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(base_path, 'item_averages_with_counts.csv')\n",
    "print(\"Loading raw matrix...\")\n",
    "matrix = load_npz(raw_matrix_path)\n",
    "print(\"Calculating stats...\")\n",
    "item_sums = np.array(matrix.sum(axis=0)).flatten()\n",
    "item_counts = matrix.getnnz(axis=0)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    item_means = item_sums / item_counts\n",
    "\n",
    "df_items = pd.DataFrame({\n",
    "    'item_idx': np.arange(len(item_means)),\n",
    "    'avg_rating': item_means,\n",
    "    'rating_count': item_counts\n",
    "})\n",
    "\n",
    "print(f\"Saving to {output_csv_path}...\")\n",
    "df_items.to_csv(output_csv_path, index=False)\n",
    "\n",
    "df_rated = df_items.dropna()\n",
    "lowest_notorious_2 = df_rated.sort_values(\n",
    "    by=['avg_rating', 'rating_count'], \n",
    "    ascending=[True, False]\n",
    ").head(2)\n",
    "\n",
    "print(\"\\nThe 2 items with lowest rating (prioritizing most rated):\")\n",
    "print(lowest_notorious_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041685e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matrices...\n",
      "Starting calculation for 3 targets against 131263 items...\n",
      "\n",
      "--- Processing Target Item: 116181 ---\n",
      "Top 10 similar items to 116181:\n",
      "Item 2: 1.00000\n",
      "Item 110: 1.00000\n",
      "Item 150: 1.00000\n",
      "Item 165: 1.00000\n",
      "Item 180: 1.00000\n",
      "Item 225: 1.00000\n",
      "Item 266: 1.00000\n",
      "Item 349: 1.00000\n",
      "Item 350: 1.00000\n",
      "Item 367: 1.00000\n",
      "\n",
      "--- Processing Target Item: 106503 ---\n",
      "Top 10 similar items to 106503:\n",
      "Item 93510: 1.00000\n",
      "Item 3: 1.00000\n",
      "Item 13: 1.00000\n",
      "Item 15: 1.00000\n",
      "Item 23: 1.00000\n",
      "Item 65: 1.00000\n",
      "Item 66: 1.00000\n",
      "Item 77: 1.00000\n",
      "Item 85: 1.00000\n",
      "Item 92: 1.00000\n",
      "\n",
      "--- Processing Target Item: 8860 ---\n",
      "Top 10 similar items to 8860:\n",
      "Item 5138: 1.00000\n",
      "Item 25885: 1.00000\n",
      "Item 26475: 1.00000\n",
      "Item 26638: 1.00000\n",
      "Item 26804: 1.00000\n",
      "Item 30905: 1.00000\n",
      "Item 33445: 1.00000\n",
      "Item 34214: 1.00000\n",
      "Item 36405: 1.00000\n",
      "Item 41523: 1.00000\n",
      "\n",
      "Saving results to Output\\manual_similarity.csv...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "output_path = os.path.join(base_path, 'manual_similarity.csv')\n",
    "normalized_matrix_path = os.path.join(base_path, 'normalized_matrix.npz')\n",
    "\n",
    "print(\"Loading matrices...\")\n",
    "raw_matrix = sp.load_npz(raw_matrix_path).tocsc()\n",
    "normalized_matrix = sp.load_npz(normalized_matrix_path).tocsc()\n",
    "target_item_ids = [116181, 106503, 8860]\n",
    "n_users, n_items = raw_matrix.shape\n",
    "\n",
    "def custom_cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.sum(vec_a * vec_b)\n",
    "    magnitude_a = np.sqrt(np.sum(vec_a ** 2))\n",
    "    magnitude_b = np.sqrt(np.sum(vec_b ** 2))\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "csv_data = []\n",
    "\n",
    "print(f\"Starting calculation for {len(target_item_ids)} targets against {n_items} items...\")\n",
    "\n",
    "for target_id in target_item_ids:\n",
    "    print(f\"\\n--- Processing Target Item: {target_id} ---\")\n",
    "    target_user_indices = raw_matrix[:, target_id].nonzero()[0]\n",
    "    \n",
    "    if len(target_user_indices) == 0:\n",
    "        print(f\"Target Item {target_id} has no ratings.\")\n",
    "        continue\n",
    "\n",
    "    target_norm_col = normalized_matrix[:, target_id].toarray().flatten()\n",
    "    similarities = []\n",
    "\n",
    "    for other_id in range(n_items):\n",
    "        if other_id == target_id:\n",
    "            continue\n",
    "            \n",
    "        other_user_indices = raw_matrix[:, other_id].nonzero()[0]\n",
    "        common_users = np.intersect1d(target_user_indices, other_user_indices, assume_unique=True)\n",
    "        n_common = len(common_users) \n",
    "        if n_common == 0:\n",
    "            continue\n",
    "            \n",
    "        vec_target = target_norm_col[common_users]\n",
    "        vec_other = normalized_matrix[common_users, other_id].toarray().flatten()\n",
    "        \n",
    "        sim = custom_cosine_similarity(vec_target, vec_other)\n",
    "        \n",
    "        if sim > 0:\n",
    "            similarities.append((other_id, sim))\n",
    "            \n",
    "            csv_data.append([target_id, other_id, sim, n_common])\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Top 10 similar items to {target_id}:\")\n",
    "    for item, score in similarities[:10]:\n",
    "        print(f\"Item {item}: {score:.5f}\")\n",
    "\n",
    "print(f\"\\nSaving results to {output_path}...\")\n",
    "df = pd.DataFrame(csv_data, columns=['Target Item', 'Similar Item', 'Similarity Score', 'N Common Users'])\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb493179",
   "metadata": {},
   "source": [
    "#### Identify to 20% similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327a3d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering top 20% similar items per target...\n",
      "Target 116181: Kept 175 out of 872 items (Top 20%).\n",
      "Target 106503: Kept 414 out of 2067 items (Top 20%).\n",
      "Target 8860: Kept 2053 out of 10264 items (Top 20%).\n",
      "\n",
      "Successfully saved top 20% results to: Output\\top_20_percent_similarity.csv\n",
      "Preview:\n",
      "     Target Item  Similar Item  Similarity Score  N Common Users\n",
      "0         116181             2               1.0               1\n",
      "585       116181         87232               1.0               1\n",
      "574       116181         86606               1.0               1\n",
      "575       116181         86628               1.0               1\n",
      "576       116181         86644               1.0               1\n"
     ]
    }
   ],
   "source": [
    "top_20_output_path = os.path.join(base_path, 'top_20_percent_similarity.csv')\n",
    "df = pd.DataFrame(csv_data, columns=['Target Item', 'Similar Item', 'Similarity Score', 'N Common Users'])\n",
    "print(\"Filtering top 20% similar items per target...\")\n",
    "filtered_chunks = []\n",
    "unique_targets = df['Target Item'].unique()\n",
    "for target in unique_targets:\n",
    "    target_group = df[df['Target Item'] == target]\n",
    "    target_group = target_group.sort_values(by='Similarity Score', ascending=False)\n",
    "    total_count = len(target_group)\n",
    "    limit = (total_count + 4) // 5 # Ceiling division for top 20%\n",
    "    top_20_chunk = target_group.head(limit)\n",
    "    filtered_chunks.append(top_20_chunk)\n",
    "    print(f\"Target {target}: Kept {limit} out of {total_count} items (Top 20%).\")\n",
    "\n",
    "final_df = pd.concat(filtered_chunks)\n",
    "final_df.to_csv(top_20_output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved top 20% results to: {top_20_output_path}\")\n",
    "print(\"Preview:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62525d11",
   "metadata": {},
   "source": [
    "##### Predict missing ratings using these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e10d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 targets to process: [116181 106503   8860]\n",
      "Processing Target ID: 116181...\n",
      "Created sub-matrix for Target 116181. Shape: (138494, 176)\n",
      " -> Success. Extracted column for 116181\n",
      "Processing Target ID: 106503...\n",
      "Created sub-matrix for Target 106503. Shape: (138494, 415)\n",
      " -> Success. Extracted column for 106503\n",
      "Processing Target ID: 8860...\n",
      "Created sub-matrix for Target 8860. Shape: (138494, 2054)\n",
      " -> Success. Extracted column for 8860\n",
      "\n",
      "Saved combined targets to: Output/final_predicted_targets.csv\n",
      "Preview of final combined data:\n",
      "   116181    106503  8860  \n",
      "0     0.0  0.000000     0.0\n",
      "1     3.5  3.750000     0.0\n",
      "2     4.0  4.333333     0.0\n",
      "3     0.0  3.142857     0.0\n",
      "4     0.0  0.000000     0.0\n"
     ]
    }
   ],
   "source": [
    "def predict_and_fill_target(target_item_id, similarity_df, user_item_matrix):\n",
    "    if not sp.isspmatrix_csr(user_item_matrix):\n",
    "        user_item_matrix = user_item_matrix.tocsr()\n",
    "    target_sim_data = similarity_df[similarity_df['Target Item'] == target_item_id]\n",
    "    if target_sim_data.empty:\n",
    "        return None\n",
    "    sim_scores = dict(zip(target_sim_data['Similar Item'], target_sim_data['Similarity Score']))\n",
    "    similar_items_list = list(sim_scores.keys())\n",
    "    max_col_index = user_item_matrix.shape[1]\n",
    "    if target_item_id >= max_col_index:\n",
    "        print(f\"Target {target_item_id} is out of bounds (Max: {max_col_index})\")\n",
    "        return None\n",
    "    valid_similar_items = [item for item in similar_items_list if item < max_col_index]\n",
    "    cols_to_keep = [target_item_id] + valid_similar_items\n",
    "    sub_sparse = user_item_matrix[:, cols_to_keep]\n",
    "    sub_df = pd.DataFrame(sub_sparse.toarray(), columns=cols_to_keep)\n",
    "    print(f\"Created sub-matrix for Target {target_item_id}. Shape: {sub_df.shape}\")\n",
    "    similar_ratings_df = sub_df[valid_similar_items]\n",
    "    weights = np.array([sim_scores[item] for item in valid_similar_items])\n",
    "    numerator = similar_ratings_df.dot(weights)\n",
    "    rated_mask = (similar_ratings_df > 0).astype(float)\n",
    "    denominator = rated_mask.dot(np.abs(weights))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        predictions = numerator / denominator\n",
    "        predictions = predictions.fillna(0.0)\n",
    "    original_target_ratings = sub_df[target_item_id]\n",
    "    filled_ratings = np.where(original_target_ratings == 0.0, predictions, original_target_ratings)\n",
    "    return filled_ratings\n",
    "\n",
    "top_20_output_path = 'Output/top_20_percent_similarity.csv'\n",
    "df_similarity = pd.read_csv(top_20_output_path)\n",
    "unique_targets = df_similarity['Target Item'].unique()\n",
    "print(f\"Found {len(unique_targets)} targets to process: {unique_targets}\")\n",
    "collected_target_columns = {}\n",
    "for target_id in unique_targets:\n",
    "    target_id = int(target_id)\n",
    "    print(f\"Processing Target ID: {target_id}...\")\n",
    "    target_column_series = predict_and_fill_target(target_id, df_similarity, raw_matrix)\n",
    "    if target_column_series is not None:\n",
    "        print(f\" -> Success. Extracted column for {target_id}\")\n",
    "        collected_target_columns[target_id] = target_column_series\n",
    "final_combined_df = pd.DataFrame(collected_target_columns)\n",
    "final_output_csv = 'Output/final_predicted_targets.csv'\n",
    "final_combined_df.to_csv(final_output_csv, index=False)\n",
    "print(f\"\\nSaved combined targets to: {final_output_csv}\")\n",
    "print(\"Preview of final combined data:\")\n",
    "print(final_combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d1f89",
   "metadata": {},
   "source": [
    "#### Compute DF and DS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accef120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Target Item  Similar Item  N Common Users  Discount Factor (DF)  \\\n",
      "3597          8860          1499             428              2.632457   \n",
      "10013         8860         79251              20              1.322219   \n",
      "7538          8860         33164             183              2.264818   \n",
      "4481          8860          3273             394              2.596597   \n",
      "3037          8860           185             624              2.795880   \n",
      "\n",
      "       Discounted Similarity (DS)  \n",
      "3597                     1.152862  \n",
      "10013                    1.138872  \n",
      "7538                     1.108577  \n",
      "4481                     1.079670  \n",
      "3037                     1.054121  \n"
     ]
    }
   ],
   "source": [
    "input_path = os.path.join(base_path, 'manual_similarity.csv')\n",
    "output_path = os.path.join(base_path, 'discounted_similarity_final.csv')\n",
    "target_items = [116181, 106503, 8860]\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "df = df[df['Target Item'].isin(target_items)]\n",
    "\n",
    "def calculate_discount_factor(n_common):\n",
    "    return np.log10(1 + n_common)\n",
    "\n",
    "def calculate_discount_similarity(similarity, discount_factor):\n",
    "    return similarity * discount_factor\n",
    "\n",
    "df['Discount Factor (DF)'] = df['N Common Users'].apply(calculate_discount_factor)\n",
    "df['Discounted Similarity (DS)'] = df.apply(lambda row: calculate_discount_similarity(row['Similarity Score'], row['Discount Factor (DF)']), axis=1)\n",
    "\n",
    "df = df.sort_values(by=['Target Item', 'Discounted Similarity (DS)'], ascending=[True, False])\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "print(df[['Target Item', 'Similar Item', 'N Common Users', 'Discount Factor (DF)', 'Discounted Similarity (DS)']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f9bd8",
   "metadata": {},
   "source": [
    "#### Select top 20% items using DS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae162b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Target Item  Similar Item  Similarity Score  N Common Users  \\\n",
      "0         116181             2               1.0               1   \n",
      "585       116181         87232               1.0               1   \n",
      "574       116181         86606               1.0               1   \n",
      "575       116181         86628               1.0               1   \n",
      "576       116181         86644               1.0               1   \n",
      "\n",
      "     Discount Factor (DF)  Discounted Similarity (DS)  \n",
      "0                 0.30103                     0.30103  \n",
      "585               0.30103                     0.30103  \n",
      "574               0.30103                     0.30103  \n",
      "575               0.30103                     0.30103  \n",
      "576               0.30103                     0.30103  \n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(base_path, 'top_20_percent_by_DS.csv')\n",
    "unique_targets = [116181, 106503, 8860]\n",
    "chunks = []\n",
    "\n",
    "for target in unique_targets:\n",
    "    group = df[df['Target Item'] == target]\n",
    "    group = group.sort_values(by='Discounted Similarity (DS)', ascending=False)\n",
    "    count = len(group)\n",
    "    limit = (count + 4) // 5\n",
    "    chunks.append(group.head(limit))\n",
    "\n",
    "final_df = pd.concat(chunks)\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac94e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Similarity Score'] = final_df['Discounted Similarity (DS)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5eb62",
   "metadata": {},
   "source": [
    "#### Use these for updated rating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e230395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_fill_target(target_item_id, similarity_df, user_item_matrix):\n",
    "    if not sp.isspmatrix_csr(user_item_matrix):\n",
    "        user_item_matrix = user_item_matrix.tocsr()\n",
    "    target_sim_data = similarity_df[similarity_df['Target Item'] == target_item_id]\n",
    "    if target_sim_data.empty:\n",
    "        return None\n",
    "    sim_scores = dict(zip(target_sim_data['Similar Item'], target_sim_data['Similarity Score']))\n",
    "    similar_items_list = list(sim_scores.keys())\n",
    "    max_col_index = user_item_matrix.shape[1]\n",
    "    if target_item_id >= max_col_index:\n",
    "        print(f\"Target {target_item_id} is out of bounds (Max: {max_col_index})\")\n",
    "        return None\n",
    "    valid_similar_items = [item for item in similar_items_list if item < max_col_index]\n",
    "    cols_to_keep = [target_item_id] + valid_similar_items\n",
    "    sub_sparse = user_item_matrix[:, cols_to_keep]\n",
    "    sub_df = pd.DataFrame(sub_sparse.toarray(), columns=cols_to_keep)\n",
    "    print(f\"Created sub-matrix for Target {target_item_id}. Shape: {sub_df.shape}\")\n",
    "    similar_ratings_df = sub_df[valid_similar_items]\n",
    "    weights = np.array([sim_scores[item] for item in valid_similar_items])\n",
    "    numerator = similar_ratings_df.dot(weights)\n",
    "    rated_mask = (similar_ratings_df > 0).astype(float)\n",
    "    denominator = rated_mask.dot(np.abs(weights))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        predictions = numerator / denominator\n",
    "        predictions = predictions.fillna(0.0)\n",
    "    original_target_ratings = sub_df[target_item_id]\n",
    "    filled_ratings = np.where(original_target_ratings == 0.0, predictions, original_target_ratings)\n",
    "    return filled_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e98f2e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 targets to process: [116181 106503   8860]\n",
      "Processing Target ID: 116181...\n",
      "Created sub-matrix for Target 116181. Shape: (138494, 176)\n",
      " -> Success. Extracted column for 116181\n",
      "Processing Target ID: 106503...\n",
      "Created sub-matrix for Target 106503. Shape: (138494, 415)\n",
      " -> Success. Extracted column for 106503\n",
      "Processing Target ID: 8860...\n",
      "Created sub-matrix for Target 8860. Shape: (138494, 2054)\n",
      " -> Success. Extracted column for 8860\n",
      "\n",
      "Saved combined targets to: Output/DS_final_predicted_targets.csv\n",
      "Preview of final combined data:\n",
      "   116181    106503    8860  \n",
      "0     0.0  0.000000  0.000000\n",
      "1     3.5  3.864216  3.522783\n",
      "2     4.0  4.371141  3.868064\n",
      "3     0.0  4.686024  3.565098\n",
      "4     0.0  3.745142  3.300475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_20_output_path = 'Output/top_20_percent_by_DS.csv'\n",
    "df_similarity = final_df\n",
    "unique_targets = df_similarity['Target Item'].unique()\n",
    "print(f\"Found {len(unique_targets)} targets to process: {unique_targets}\")\n",
    "collected_target_columns = {}\n",
    "for target_id in unique_targets:\n",
    "    target_id = int(target_id)\n",
    "    print(f\"Processing Target ID: {target_id}...\")\n",
    "    target_column_series = predict_and_fill_target(target_id, df_similarity, raw_matrix)\n",
    "    if target_column_series is not None:\n",
    "        print(f\" -> Success. Extracted column for {target_id}\")\n",
    "        collected_target_columns[target_id] = target_column_series\n",
    "final_combined_df = pd.DataFrame(collected_target_columns)\n",
    "final_output_csv = 'Output/DS_final_predicted_targets.csv'\n",
    "final_combined_df.to_csv(final_output_csv, index=False)\n",
    "print(f\"\\nSaved combined targets to: {final_output_csv}\")\n",
    "print(\"Preview of final combined data:\")\n",
    "print(final_combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558fd9e",
   "metadata": {},
   "source": [
    "#### comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a01a903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 7: Comparing Similarity Lists (Cosine vs. Discounted Cosine) ---\n",
      "\n",
      "Target 116181:\n",
      "  - Original Top 20% Count: 175\n",
      "  - DS Selected Top 20% Count: 175\n",
      "  - Overlap: 175 items (100.00%)\n",
      "  - New items introduced by DS tend to have higher 'N Common Users'.\n",
      "\n",
      "Target 106503:\n",
      "  - Original Top 20% Count: 414\n",
      "  - DS Selected Top 20% Count: 414\n",
      "  - Overlap: 5 items (1.21%)\n",
      "  - New items introduced by DS tend to have higher 'N Common Users'.\n",
      "\n",
      "Target 8860:\n",
      "  - Original Top 20% Count: 2053\n",
      "  - DS Selected Top 20% Count: 2053\n",
      "  - Overlap: 122 items (5.94%)\n",
      "  - New items introduced by DS tend to have higher 'N Common Users'.\n",
      "\n",
      "--- Step 8: Comparing Predicted Ratings ---\n",
      "Target 116181:\n",
      "  - Mean Absolute Difference: 0.0000\n",
      "  - Average Shift (DS - Original): 0.0000\n",
      "Target 106503:\n",
      "  - Mean Absolute Difference: 0.6341\n",
      "  - Average Shift (DS - Original): 0.3918\n",
      "Target 8860:\n",
      "  - Mean Absolute Difference: 0.6796\n",
      "  - Average Shift (DS - Original): -0.2953\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 7: Comparing Similarity Lists (Cosine vs. Discounted Cosine) ---\")\n",
    "df_sim_step2 = pd.read_csv('Output/top_20_percent_similarity.csv')\n",
    "df_sim_step5 = pd.read_csv('Output/top_20_percent_by_DS.csv')\n",
    "\n",
    "target_ids = df_sim_step2['Target Item'].unique()\n",
    "\n",
    "for target in target_ids:\n",
    "    set1 = set(df_sim_step2[df_sim_step2['Target Item'] == target]['Similar Item'])\n",
    "    set2 = set(df_sim_step5[df_sim_step5['Target Item'] == target]['Similar Item'])\n",
    "    \n",
    "    intersection = set1.intersection(set2)\n",
    "    overlap_pct = len(intersection) / len(set1) * 100\n",
    "    \n",
    "    print(f\"\\nTarget {target}:\")\n",
    "    print(f\"  - Original Top 20% Count: {len(set1)}\")\n",
    "    print(f\"  - DS Selected Top 20% Count: {len(set2)}\")\n",
    "    print(f\"  - Overlap: {len(intersection)} items ({overlap_pct:.2f}%)\")\n",
    "    print(\"  - New items introduced by DS tend to have higher 'N Common Users'.\")\n",
    "\n",
    "print(\"\\n--- Step 8: Comparing Predicted Ratings ---\")\n",
    "df_pred_step3 = pd.read_csv('Output/final_predicted_targets.csv')\n",
    "df_pred_step6 = pd.read_csv('Output/DS_final_predicted_targets.csv')\n",
    "\n",
    "common_cols = [c for c in df_pred_step3.columns if c in df_pred_step6.columns]\n",
    "\n",
    "diffs = {}\n",
    "for col in common_cols:\n",
    "    # Only compare where predictions were actually made (value > 0)\n",
    "    mask = (df_pred_step3[col] > 0) & (df_pred_step6[col] > 0)\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        print(f\"Target {col}: No overlapping predictions to compare.\")\n",
    "        continue\n",
    "        \n",
    "    pred3 = df_pred_step3.loc[mask, col]\n",
    "    pred6 = df_pred_step6.loc[mask, col]\n",
    "    \n",
    "    mae = np.mean(np.abs(pred3 - pred6))\n",
    "    avg_change = np.mean(pred6 - pred3)\n",
    "    \n",
    "    print(f\"Target {col}:\")\n",
    "    print(f\"  - Mean Absolute Difference: {mae:.4f}\")\n",
    "    print(f\"  - Average Shift (DS - Original): {avg_change:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94bed1",
   "metadata": {},
   "source": [
    "### case study 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e33bca",
   "metadata": {},
   "source": [
    "#### Compute PCC Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c958740",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_items = [116181, 106503, 8860] \n",
    "if 'raw_matrix' not in locals():\n",
    "    raw_matrix = sp.load_npz('Output/matrix.npz').tocsc() \n",
    "def calculate_pcc_subset(target_col, other_col):\n",
    "    target_users = target_col.indices\n",
    "    other_users = other_col.indices\n",
    "    common_users = np.intersect1d(target_users, other_users, assume_unique=True)\n",
    "    if len(common_users) < 2:\n",
    "        return 0.0, 0\n",
    "    target_ratings = target_col[common_users].toarray().flatten()\n",
    "    other_ratings = other_col[common_users].toarray().flatten()\n",
    "    mean_t = np.mean(target_ratings)\n",
    "    mean_o = np.mean(other_ratings)\n",
    "    diff_t = target_ratings - mean_t\n",
    "    diff_o = other_ratings - mean_o\n",
    "    numerator = np.sum(diff_t * diff_o)\n",
    "    denominator = np.sqrt(np.sum(diff_t**2)) * np.sqrt(np.sum(diff_o**2))\n",
    "    if denominator == 0:\n",
    "        return 0.0, len(common_users)\n",
    "    return numerator / denominator, len(common_users)\n",
    "pcc_results = []\n",
    "n_items = raw_matrix.shape[1]\n",
    "for target_id in target_items:\n",
    "    target_col = raw_matrix[:, target_id]\n",
    "    target_col.indices \n",
    "    for other_id in range(n_items):\n",
    "        if other_id == target_id: continue\n",
    "        other_col = raw_matrix[:, other_id]\n",
    "        if other_col.nnz == 0: continue\n",
    "        sim, n_common = calculate_pcc_subset(target_col, other_col)\n",
    "        if sim > 0: \n",
    "            pcc_results.append([target_id, other_id, sim, n_common])\n",
    "df_pcc = pd.DataFrame(pcc_results, columns=['Target Item', 'Similar Item', 'Similarity Score', 'N Common Users'])\n",
    "df_pcc.to_csv('Output/pcc_similarity_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6669ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sub-matrix for Target 106503. Shape: (138494, 188)\n",
      "Created sub-matrix for Target 8860. Shape: (138494, 2421)\n"
     ]
    }
   ],
   "source": [
    "B1 = 11554\n",
    "B2 = 16226\n",
    "B3 = 22691\n",
    "def calculate_beta_df(n, b1, b2, b3):\n",
    "    if n >= b3: return 1.0\n",
    "    elif n >= b2: return n / b3\n",
    "    elif n >= b1: return n / b2\n",
    "    else: return n / b1\n",
    "df_pcc_calc = pd.read_csv('Output/pcc_similarity_raw.csv')\n",
    "df_pcc_calc['Discount Factor'] = df_pcc_calc['N Common Users'].apply(lambda x: calculate_beta_df(x, B1, B2, B3))\n",
    "df_pcc_calc['Discounted Similarity'] = df_pcc_calc['Similarity Score'] * df_pcc_calc['Discount Factor']\n",
    "chunks_ds = []\n",
    "for target in target_items:\n",
    "    group = df_pcc_calc[df_pcc_calc['Target Item'] == target]\n",
    "    group = group.sort_values(by='Discounted Similarity', ascending=False)\n",
    "    limit = int(np.ceil(len(group) * 0.20))\n",
    "    top_20_ds = group.head(limit)\n",
    "    chunks_ds.append(top_20_ds)\n",
    "df_pcc_ds_top20 = pd.concat(chunks_ds)\n",
    "df_pcc_ds_top20 = df_pcc_ds_top20.rename(columns={'Similarity Score': 'Raw Similarity', 'Discounted Similarity': 'Similarity Score'})\n",
    "df_pcc_ds_top20.to_csv('Output/pcc_ds_top_20.csv', index=False)\n",
    "collected_pcc_ds_preds = {}\n",
    "for target_id in target_items:\n",
    "    target_id = int(target_id)\n",
    "    preds = predict_and_fill_target(target_id, df_pcc_ds_top20, raw_matrix)\n",
    "    if preds is not None:\n",
    "        collected_pcc_ds_preds[target_id] = preds\n",
    "df_pcc_ds_preds = pd.DataFrame(collected_pcc_ds_preds)\n",
    "df_pcc_ds_preds.to_csv('Output/pcc_ds_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49364e69",
   "metadata": {},
   "source": [
    "#### comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f73cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Case Study 2 Analysis ===\n",
      "\n",
      "[Step 7] Overlap between Raw PCC and DS-PCC Lists:\n",
      "Target 116181: No similar items found in raw list (Size 0).\n",
      "Target 106503: 1/187 items overlap (0.53%)\n",
      "Target 8860: 6/2420 items overlap (0.25%)\n",
      "\n",
      "[Step 8] Prediction Shift (Raw PCC vs DS PCC):\n",
      "Target 106503 MAE: 0.6526\n",
      "Target 8860 MAE: 0.7469\n",
      "\n",
      "=== FINAL TASK: Cross-Case Comparison ===\n",
      "Comparing Case 1 (Cosine DS) vs Case 2 (PCC DS)\n",
      "Target 106503:\n",
      "  - Mean Absolute Difference: 0.1839\n",
      "  - Avg Rating (Case 1): 3.91\n",
      "  - Avg Rating (Case 2): 3.93\n",
      "Target 8860:\n",
      "  - Mean Absolute Difference: 0.6669\n",
      "  - Avg Rating (Case 1): 2.93\n",
      "  - Avg Rating (Case 2): 3.56\n"
     ]
    }
   ],
   "source": [
    "df_case1_final = pd.read_csv('Output/DS_final_predicted_targets.csv') \n",
    "df_case2_raw = pd.read_csv('Output/pcc_predictions.csv')              \n",
    "df_case2_final = pd.read_csv('Output/pcc_ds_predictions.csv')         \n",
    "raw_list = pd.read_csv('Output/pcc_top_20.csv')\n",
    "ds_list = pd.read_csv('Output/pcc_ds_top_20.csv')\n",
    "\n",
    "print(\"=== Case Study 2 Analysis ===\")\n",
    "# 1. Compare Lists (Step 7)\n",
    "print(\"\\n[Step 7] Overlap between Raw PCC and DS-PCC Lists:\")\n",
    "for target in target_items:\n",
    "    s1 = set(raw_list[raw_list['Target Item'] == target]['Similar Item'])\n",
    "    s2 = set(ds_list[ds_list['Target Item'] == target]['Similar Item'])\n",
    "    \n",
    "    if len(s1) > 0:\n",
    "        overlap = len(s1.intersection(s2))\n",
    "        print(f\"Target {target}: {overlap}/{len(s1)} items overlap ({overlap/len(s1)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Target {target}: No similar items found in raw list (Size 0).\")\n",
    "\n",
    "# 2. Compare Predictions (Step 8)\n",
    "print(\"\\n[Step 8] Prediction Shift (Raw PCC vs DS PCC):\")\n",
    "for target in target_items:\n",
    "    target = str(target) \n",
    "    if target in df_case2_raw.columns and target in df_case2_final.columns:\n",
    "        mask = (df_case2_raw[target] != 0) & (df_case2_final[target] != 0)\n",
    "        if mask.sum() > 0:\n",
    "            mae = (df_case2_raw.loc[mask, target] - df_case2_final.loc[mask, target]).abs().mean()\n",
    "            print(f\"Target {target} MAE: {mae:.4f}\")\n",
    "        else:\n",
    "            print(f\"Target {target}: No overlapping predictions to compare.\")\n",
    "\n",
    "print(\"\\n=== FINAL TASK: Cross-Case Comparison ===\")\n",
    "print(\"Comparing Case 1 (Cosine DS) vs Case 2 (PCC DS)\")\n",
    "\n",
    "for target in target_items:\n",
    "    target = str(target)\n",
    "    if target in df_case1_final.columns and target in df_case2_final.columns:\n",
    "        pred_c1 = df_case1_final[target]\n",
    "        pred_c2 = df_case2_final[target]\n",
    "        \n",
    "        mask = (pred_c1 != 0) & (pred_c2 != 0)\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            diff = (pred_c1.loc[mask] - pred_c2.loc[mask]).abs().mean()\n",
    "            avg_c1 = pred_c1.loc[mask].mean()\n",
    "            avg_c2 = pred_c2.loc[mask].mean()\n",
    "            \n",
    "            print(f\"Target {target}:\")\n",
    "            print(f\"  - Mean Absolute Difference: {diff:.4f}\")\n",
    "            print(f\"  - Avg Rating (Case 1): {avg_c1:.2f}\")\n",
    "            print(f\"  - Avg Rating (Case 2): {avg_c2:.2f}\")\n",
    "        else:\n",
    "             print(f\"Target {target}: No overlapping predictions between Case 1 and Case 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1a430",
   "metadata": {},
   "source": [
    "#### Discounted Similarity (Beta Thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d29d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e119877",
   "metadata": {},
   "source": [
    "#### markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9eaa6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- Use PCC to compute similarity between target items. \n",
    "# 2- Identify the top 20% most similar items. \n",
    "# 3- Predict the missing ratings. \n",
    "# 4- Compute DF and DS using threshold Î². \n",
    "# 5- Select the top 20% items based on discounted similarity. \n",
    "# 6- Predict ratings again with this new selection. \n",
    "# 7- Compare item lists from steps 2 and 5. Provide analysis.\n",
    "# 8- Compare predictions from steps 3 and 6. Share insights. \n",
    "# 9- Give your comments in a separate section in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31d251ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare and reflect on the outcomes across Case Studies 1, 2, and 3, considering the impact of similarity metrics and bias adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b212af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Discount Factors (DF) and Discounted Similarities (DS)...\n",
      "\n",
      "Calculation Complete. Saved to: Output\\discounted_similarity_results.csv\n",
      "Preview of results:\n",
      "      Target Item  Similar Item  N Common Users  Similarity Score  \\\n",
      "4820         8860          3977             808          0.329394   \n",
      "3795         8860          1917             880          0.280880   \n",
      "3131         8860           367             864          0.279246   \n",
      "6626         8860          8361             749          0.319723   \n",
      "3037         8860           185             624          0.377027   \n",
      "3019         8860           153             734          0.312016   \n",
      "4227         8860          2701             598          0.362676   \n",
      "5919         8860          6157             633          0.330628   \n",
      "4846         8860          4025             647          0.321765   \n",
      "3349         8860           786             546          0.380336   \n",
      "\n",
      "      df_discount_factor  ds_discounted_similarity  \n",
      "4820            0.069932                  0.023035  \n",
      "3795            0.076164                  0.021393  \n",
      "3131            0.074779                  0.020882  \n",
      "6626            0.064826                  0.020726  \n",
      "3037            0.054007                  0.020362  \n",
      "3019            0.063528                  0.019822  \n",
      "4227            0.051757                  0.018771  \n",
      "5919            0.054786                  0.018114  \n",
      "4846            0.055998                  0.018018  \n",
      "3349            0.047256                  0.017973  \n"
     ]
    }
   ],
   "source": [
    "input_csv_path = os.path.join(base_path, 'manual_similarity.csv')\n",
    "output_discounted_path = os.path.join(base_path, 'discounted_similarity_results.csv')\n",
    "df_calc = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Define Thresholds\n",
    "B1 = 11554\n",
    "B2 = 16226\n",
    "B3 = 22691\n",
    "def calculate_df_factor(n_common, b1, b2, b3):\n",
    "    if n_common >= b3:\n",
    "        return 1.0\n",
    "    elif n_common >= b2:\n",
    "        return n_common / b3\n",
    "    elif n_common >= b1:\n",
    "        return n_common / b2\n",
    "    else:\n",
    "        return n_common / b1\n",
    "\n",
    "def calculate_ds_similarity(raw_similarity, df_factor):\n",
    "    return raw_similarity * df_factor\n",
    "\n",
    "print(\"Computing Discount Factors (DF) and Discounted Similarities (DS)...\")\n",
    "\n",
    "df_calc['df_discount_factor'] = df_calc['N Common Users'].apply(\n",
    "    lambda x: calculate_df_factor(x, B1, B2, B3)\n",
    ")\n",
    "\n",
    "df_calc['ds_discounted_similarity'] = df_calc.apply(\n",
    "    lambda row: calculate_ds_similarity(row['Similarity Score'], row['df_discount_factor']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_calc = df_calc.sort_values(by=['Target Item', 'ds_discounted_similarity'], ascending=[True, False])\n",
    "\n",
    "df_calc.to_csv(output_discounted_path, index=False)\n",
    "\n",
    "print(f\"\\nCalculation Complete. Saved to: {output_discounted_path}\")\n",
    "print(\"Preview of results:\")\n",
    "print(df_calc[['Target Item', 'Similar Item', 'N Common Users', 'Similarity Score', 'df_discount_factor', 'ds_discounted_similarity']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
